{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06-NLP_Demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNzK2XRJ1wZitUev+bXQpqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlaFalaki/workshop-materials/blob/main/2022-practical-deep-learning/06-NLP_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Welcome to* **\"Practical Deep Learning\"** *workshop!*\n",
        "\n",
        "My name is **Ala Alam Falaki** *(Ph.D. Candidate @ UofW)*\n",
        "\n",
        "Research Interest:\n",
        "\n",
        "\n",
        "\n",
        "*   Natural Language Processing\n",
        "*   Generative Models\n",
        "*   Automatic Text Summarization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "br-p0ArGPXRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ The matterials are available on my [Github account](https://github.com/AlaFalaki/workshop-materials). You can open notebooks in the Google Colab environment and easily run them."
      ],
      "metadata": {
        "id": "Nh8rR9tAPYJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo (Natural Language Processing)\n",
        "\n",
        "* Learn how to:\n",
        " * Load a pre-trained model\n",
        " * Use the [**Gradio**](https://gradio.app/) platform for prototyping\n",
        " * Use Google Colab to host your model"
      ],
      "metadata": {
        "id": "2rAjfpYUQFny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install FastAI2 and Gradio"
      ],
      "metadata": {
        "id": "LIYTzHY4RoPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Run the cell below to install both FastAI2 and Gradio. <br /><br />\n",
        "> ⚠️ Make sure to restart the current runtime after the installation for changes to affect. Select 'Runtime' From the top menu and click on 'Restart Runtime'."
      ],
      "metadata": {
        "id": "_ghkcDELR8FZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14j3IsNONoD-",
        "outputId": "42da0938-6f47-4bb4-de68-810af71f8c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 189 kB 24.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-2.8.5-py3-none-any.whl (649 kB)\n",
            "\u001b[K     |████████████████████████████████| 649 kB 17.2 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.74.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.17.5-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting orjson\n",
            "  Downloading orjson-3.6.7-cp37-cp37m-manylinux_2_24_x86_64.whl (255 kB)\n",
            "\u001b[K     |████████████████████████████████| 255 kB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 71.4 MB/s \n",
            "\u001b[?25hCollecting paramiko\n",
            "  Downloading paramiko-2.9.2-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 78.1 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.0.1-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 67.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 57.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (3.10.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Collecting starlette==0.17.1\n",
            "  Downloading starlette-0.17.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 55.6 MB/s \n",
            "\u001b[?25hCollecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2018.9)\n",
            "Collecting cryptography>=2.5\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 54.6 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 576 kB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n",
            "Collecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.5.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=0bbb27dc02547d2d989e40fae4f82ce18520093d6fc1328c0ea3a65491d73b92\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=5e45c0263de128d785fe662d99e7710e678a42a53144cd677c463eaf47b339cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, multidict, markdown-it-py, frozenlist, anyio, yarl, starlette, pynacl, pydantic, monotonic, mdit-py-plugins, linkify-it-py, h11, cryptography, bcrypt, backoff, asynctest, async-timeout, asgiref, aiosignal, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, ffmpy, fastapi, analytics-python, aiohttp, gradio\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 analytics-python-1.4.0 anyio-3.5.0 asgiref-3.5.0 async-timeout-4.0.2 asynctest-0.13.0 backoff-1.10.0 bcrypt-3.2.0 cryptography-36.0.1 fastapi-0.74.1 ffmpy-0.3.0 frozenlist-1.3.0 gradio-2.8.5 h11-0.13.0 linkify-it-py-1.0.3 markdown-it-py-2.0.1 mdit-py-plugins-0.3.0 mdurl-0.1.0 monotonic-1.6 multidict-6.0.2 orjson-3.6.7 paramiko-2.9.2 pycryptodome-3.14.1 pydantic-1.9.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 sniffio-1.2.0 starlette-0.17.1 uc-micro-py-1.0.1 uvicorn-0.17.5 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -Uq fastai\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text.all import *\n",
        "import gradio as gr\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "LhEK5yuJTa59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Model"
      ],
      "metadata": {
        "id": "kBIkInqCT0-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> You can download the model used in the demo from [this link](https://drive.google.com/file/d/1SKPZSr_doT9i88T4LQcqgM1mlHOwVL_3/view?usp=sharing) (nlp_finetuned_cls.pkl file, do not uncompress it) and upload it to this Colab instance by chosing the Files menu from the left sidebar. It is the same model that we trained in \"01-NLP_Part1\" notebook."
      ],
      "metadata": {
        "id": "bIOTHF76VdKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_learner(\"nlp_finetuned_cls\")"
      ],
      "metadata": {
        "id": "XTvJ9h8DT1Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Gradio"
      ],
      "metadata": {
        "id": "ANW_7XxBXwlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> First step is to define a function that recieves an input text and return the sentiment using the loaded model. It should return either \"Positive\" or \"Negative\" for a given review."
      ],
      "metadata": {
        "id": "-kNG3uavZ17t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(review, model=model):\n",
        "\n",
        "  pred = model.predict( review )\n",
        "\n",
        "  if pred[0] == '2':\n",
        "    return \"POSITIVE\"\n",
        "\n",
        "  return \"NEGATIVE\""
      ],
      "metadata": {
        "id": "OXxbwwHEZ2ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> We already imported the Gradio library, so the next step is to use the `Interface` class to pass the prediction function, and the input, output types. The platform will build an interface for you with a public URL so you can show-off your work. In the code below we define both input, and output as text type and use the `launch()` method to, well, lauch the application."
      ],
      "metadata": {
        "id": "b7oz-SexX04V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(fn=predictor, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "xrkZ9LNvrXdE",
        "outputId": "864ecdd9-b6f6-4a3d-ff1c-70b935946ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://47524.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://47524.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7ff7abb1b6d0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<fastapi.applications.FastAPI at 0x7ff7b2201910>,\n",
              " 'http://127.0.0.1:7861/',\n",
              " 'https://47524.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a bit more interesting UI"
      ],
      "metadata": {
        "id": "ZE4LE2Xbttqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The Gradio platform is really felixible. It lets you present your work no matter what the area is! Your input could be image, voice, text, ... that are listed in their [documentation](https://gradio.app/docs/).\n",
        ">\n",
        "> I am going to use \"Chatbox\" interface that recieves a input text, and send the response back. The sentiment analysis might not be the perfect application for the mentioned interface, but we just want to showcase a more advanced UI...\n",
        ">\n",
        "> We need to start by re-writing the `predictor` function. It should accept an input variable named `history` that will holds the previous state of conversation. We need to add our current interaction to the history list and return it to the interface."
      ],
      "metadata": {
        "id": "hZRrgoVcttpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictor(review, history, model=model):\n",
        "  history = history or []\n",
        "\n",
        "  pred = model.predict( review )\n",
        "\n",
        "  if pred[0] == '2':\n",
        "    res = \"POSITIVE\"\n",
        "  else:\n",
        "    res = \"NEGATIVE\"\n",
        "\n",
        "  history.append((review, res))\n",
        "\n",
        "  return history, history"
      ],
      "metadata": {
        "id": "1RO4TBoS1X9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Also, I changed the input type from just a text field, to a Textbox (which basically is a larger input area), and ask Gradio to show the output as a \"chatbot\". We can also pass a Title, and description to the `Interface` method to maybe add some guidelines for the end-user."
      ],
      "metadata": {
        "id": "WCuUHtFJ7DEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(fn=predictor,\n",
        "                     inputs=[gr.inputs.Textbox(lines=4, placeholder=\"Write a Review Here...\"), \"state\"],\n",
        "                     outputs=[\"chatbot\", \"state\"],\n",
        "                     allow_flagging=\"never\",\n",
        "                     title=\"A Sentiment Analysis App\",\n",
        "                     description=\"You can submit any text in the field below to see the text's sentiment.\")\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "6GzJBy2WttKL",
        "outputId": "6553744e-fb74-465e-87cc-d8c869ea9899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://12610.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://12610.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7ff7a6fe05d0>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<fastapi.applications.FastAPI at 0x7ff7b2201910>,\n",
              " 'http://127.0.0.1:7865/',\n",
              " 'https://12610.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lastly,"
      ],
      "metadata": {
        "id": "dmGiJZo26K5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Gradio will let you host the application for 72 hours for free! You also have the option to use the [Huggingface Spaces](https://huggingface.co/spaces) to permanently host your model.\n",
        ">\n",
        "> I highly recommend you to read the [Getting Started](https://gradio.app/getting_started/), and [Documentation](https://gradio.app/docs/) section to learn more about the platform and its different interfaces."
      ],
      "metadata": {
        "id": "F69SyfqX6Pyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop Resources"
      ],
      "metadata": {
        "id": "HpLpAj2t990I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The [Github Repository](https://github.com/AlaFalaki/workshop-materials/) contains all the notebooks and materials presented in this workshop."
      ],
      "metadata": {
        "id": "_lVWrnH99__f"
      }
    }
  ]
}